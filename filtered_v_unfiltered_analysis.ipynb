{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import iris.plot as iplt\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALL YEARS COMBINED FOR PLOTS ###\n",
    "### ---------------------------- ###\n",
    "####################################\n",
    "\n",
    "### STARTING WITH THE ORIGINAL (UNFILTERED) TRACKS ###  \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "year = 1998\n",
    "file = '/data/users/hgilmour/initial_tracks/tobac_initial_tracks/tracking/tracks_{}.h5'.format(year)\n",
    "original_1998 = pd.read_hdf(file, 'table')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(original_1998.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"1998\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    original_1998['cell'] = original_1998['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(original_1998.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = 1999\n",
    "file = '/data/users/hgilmour/initial_tracks/tobac_initial_tracks/tracking/tracks_{}.h5'.format(year)\n",
    "original_1999 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(original_1999.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"1999\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    original_1999['cell'] = original_1999['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(original_1999.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = 2000\n",
    "file = '/data/users/hgilmour/initial_tracks/tobac_initial_tracks/tracking/tracks_{}.h5'.format(year)\n",
    "original_2000 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(original_2000.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2000\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    original_2000['cell'] = original_2000['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(original_2000.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = 2001\n",
    "file = '/data/users/hgilmour/initial_tracks/tobac_initial_tracks/tracking/tracks_{}.h5'.format(year)\n",
    "original_2001 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(original_2001.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2001\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    original_2001['cell'] = original_2001['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(original_2001.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = 2002\n",
    "file = '/data/users/hgilmour/initial_tracks/tobac_initial_tracks/tracking/tracks_{}.h5'.format(year)\n",
    "original_2002 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(original_2002.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2002\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    original_2002['cell'] = original_2002['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(original_2002.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = 2003\n",
    "file = '/data/users/hgilmour/initial_tracks/tobac_initial_tracks/tracking/tracks_{}.h5'.format(year)\n",
    "original_2003 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(original_2003.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2003\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    original_2003['cell'] = original_2003['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(original_2003.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = 2004\n",
    "file = '/data/users/hgilmour/initial_tracks/tobac_initial_tracks/tracking/tracks_{}.h5'.format(year)\n",
    "original_2004 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(original_2004.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2004\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    original_2004['cell'] = original_2004['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(original_2004.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = 2005\n",
    "file = '/data/users/hgilmour/initial_tracks/tobac_initial_tracks/tracking/tracks_{}.h5'.format(year)\n",
    "original_2005 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(original_2005.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2005\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    original_2005['cell'] = original_2005['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(original_2005.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = 2006\n",
    "file = '/data/users/hgilmour/initial_tracks/tobac_initial_tracks/tracking/tracks_{}.h5'.format(year)\n",
    "original_2006 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(original_2006.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2006\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    original_2006['cell'] = original_2006['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(original_2006.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "year = 2007\n",
    "file = '/data/users/hgilmour/initial_tracks/tobac_initial_tracks/tracking/tracks_{}.h5'.format(year)\n",
    "original_2007 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(original_2007.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2007\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    original_2007['cell'] = original_2007['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(original_2007.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_original = pd.DataFrame()\n",
    "\n",
    "all_years_original = pd.concat([all_years_original, original_1998, original_1999, original_2000, original_2001, original_2002, original_2003, original_2004, original_2005, original_2006, original_2007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(all_years_original.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOW THE FINAL (FILTERED) TRACKS ###\n",
    "\n",
    "year = 1998\n",
    "file = '/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/{}/CCPF_{}.hdf'.format(year, year)\n",
    "CCPF_1998 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(CCPF_1998.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"1998\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    CCPF_1998['cell'] = CCPF_1998['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(CCPF_1998.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1999\n",
    "file = '/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/{}/CCPF_{}.hdf'.format(year, year)\n",
    "CCPF_1999 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(CCPF_1999.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"1999\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    CCPF_1999['cell'] = CCPF_1999['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(CCPF_1999.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2000\n",
    "file = '/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/{}/CCPF_{}.hdf'.format(year, year)\n",
    "CCPF_2000 = pd.read_hdf(file, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(CCPF_2000.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2000\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    CCPF_2000['cell'] = CCPF_2000['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(CCPF_2000.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2001\n",
    "file = '/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/{}/CCPF_{}.hdf'.format(year, year)\n",
    "CCPF_2001 = pd.read_hdf(file, 'table')\n",
    "\n",
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(CCPF_2001.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2001\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    CCPF_2001['cell'] = CCPF_2001['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(CCPF_2001.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2002\n",
    "file = '/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/{}/CCPF_{}.hdf'.format(year, year)\n",
    "CCPF_2002 = pd.read_hdf(file, 'table')\n",
    "\n",
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(CCPF_2002.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2002\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    CCPF_2002['cell'] = CCPF_2002['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(CCPF_2002.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2003\n",
    "file = '/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/{}/CCPF_{}.hdf'.format(year, year)\n",
    "CCPF_2003 = pd.read_hdf(file, 'table')\n",
    "\n",
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(CCPF_2003.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2003\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    CCPF_2003['cell'] = CCPF_2003['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(CCPF_2003.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2004\n",
    "file = '/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/{}/CCPF_{}.hdf'.format(year, year)\n",
    "CCPF_2004 = pd.read_hdf(file, 'table')\n",
    "\n",
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(CCPF_2004.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2004\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    CCPF_2004['cell'] = CCPF_2004['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(CCPF_2004.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2005\n",
    "file = '/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/{}/CCPF_{}.hdf'.format(year, year)\n",
    "CCPF_2005 = pd.read_hdf(file, 'table')\n",
    "\n",
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(CCPF_2005.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2005\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    CCPF_2005['cell'] = CCPF_2005['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(CCPF_2005.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2006\n",
    "file = '/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/{}/CCPF_{}.hdf'.format(year, year)\n",
    "CCPF_2006 = pd.read_hdf(file, 'table')\n",
    "\n",
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(CCPF_2006.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2006\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    CCPF_2006['cell'] = CCPF_2006['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(CCPF_2006.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2007\n",
    "file = '/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/{}/CCPF_{}.hdf'.format(year, year)\n",
    "CCPF_2007 = pd.read_hdf(file, 'table')\n",
    "\n",
    "# adding the year to the start of the cell number so that when all the yearly\n",
    "# dataframes are combined into 1, there are no duplicate cell numbers\n",
    "\n",
    "for cell in np.unique(CCPF_2007.cell.values):\n",
    "    if cell < 0:\n",
    "        continue\n",
    "    else:\n",
    "        new_cell = int(\"2007\" + str(cell))\n",
    "        #print(new_cell)\n",
    "    CCPF_2007['cell'] = CCPF_2007['cell'].replace(cell, new_cell)\n",
    "\n",
    "print(np.unique(CCPF_2007.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_CCPF = pd.DataFrame()\n",
    "\n",
    "all_years_CCPF = pd.concat([all_years_CCPF, CCPF_1998, CCPF_1999, CCPF_2000, CCPF_2001, CCPF_2002, CCPF_2003, CCPF_2004, CCPF_2005, CCPF_2006, CCPF_2007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_CCPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_CCPF.sort_values(by=['timestr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(all_years_CCPF.cell.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_CCPF.to_hdf('/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/all_years_CCPF.hdf', 'table')\n",
    "\n",
    "all_years_original.to_hdf('/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/all_years_original.hdf', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_original = pd.read_hdf('/project/cssp_brazil/mcs_tracking_HG/final_tracks_CPM/merged/all_years_original.hdf', 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_original.time.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stats for pre and post filtering ##\n",
    "# ---------------------------------------\n",
    "print('Stats for all years combined:')\n",
    "print(' ')\n",
    "\n",
    "print(np.unique(all_years_original.cell.values).shape[0], \"cells in the original non-filtered dataset\")\n",
    "\n",
    "print(np.unique(all_years_CCPF.cell.values).shape[0], \"cells that met both criteria post-filtering\")\n",
    "\n",
    "print(\"Total of\", (np.unique(all_years_original.cell.values).shape[0])-(np.unique(all_years_CCPF.cell.values).shape[0]), \"cells removed post-filtering\")\n",
    "\n",
    "print(((((np.unique(all_years_original.cell.values).shape[0])-(np.unique(all_years_CCPF.cell.values).shape[0]))/(np.unique(all_years_original.cell.values).shape[0]))*100), \"% decrease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes= [all_years_CCPF, all_years_original]\n",
    "\n",
    "for df in dataframes:\n",
    "    df.timestr = df.timestr.astype(str)\n",
    "    df['datetime'] = pd.to_datetime(df.timestr.str.split(',\\s*').str[0]) \n",
    "    set(df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of initiation and dissipation times\n",
    "# ========================================\n",
    "import functions\n",
    "\n",
    "#filtered (CC=210K)\n",
    "init_hours_filtered = functions.get_mcs_init(all_years_CCPF)\n",
    "init_hours_filtered = np.array(init_hours_filtered)\n",
    "init_hours_filtered, bins = np.histogram(init_hours_filtered, bins = np.arange(0,25))\n",
    "\n",
    "diss_hours_filtered = functions.get_mcs_diss(all_years_CCPF)\n",
    "diss_hours_filtered = np.array(diss_hours_filtered)\n",
    "diss_hours_filtered, bins = np.histogram(diss_hours_filtered, bins = np.arange(0,25))\n",
    "\n",
    "init_filtered = init_hours_filtered/np.nansum(init_hours_filtered) * 100\n",
    "diss_filtered = diss_hours_filtered/np.nansum(diss_hours_filtered) * 100\n",
    "\n",
    "max_hours_rain_filtered = functions.get_mcs_max_precip(all_years_CCPF)\n",
    "\n",
    "max_hours_tb_filtered = functions.get_mcs_min_tb(all_years_CCPF)\n",
    "\n",
    "\n",
    "#unfiltered\n",
    "init_hours_unfiltered = functions.get_mcs_init(all_years_original)\n",
    "init_hours_unfiltered = np.array(init_hours_unfiltered)\n",
    "init_hours_unfiltered, bins = np.histogram(init_hours_unfiltered, bins = np.arange(0,25))\n",
    "\n",
    "diss_hours_unfiltered = functions.get_mcs_diss(all_years_original)\n",
    "diss_hours_unfiltered = np.array(diss_hours_unfiltered)\n",
    "diss_hours_unfiltered, bins = np.histogram(diss_hours_unfiltered, bins = np.arange(0,25))\n",
    "\n",
    "init_unfiltered = init_hours_unfiltered/np.nansum(init_hours_unfiltered) * 100\n",
    "diss_unfiltered = diss_hours_unfiltered/np.nansum(diss_hours_unfiltered) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of initiation time of MCSs \n",
    "import seaborn as sns \n",
    "sns.set()\n",
    "\n",
    "plt.figure(figsize=(26,20))\n",
    "f= 30\n",
    "lf= 30\n",
    "l=32\n",
    "\n",
    "ax1 = plt.subplot(2, 1, 1) \n",
    "ax1.set_title('a) Original (unfiltered) ', loc='left', fontsize= 35 )\n",
    "ax1.plot(np.arange(0,24), init_unfiltered, label = 'initiation ', color= 'darkblue', linewidth = 3)\n",
    "ax1.plot(np.arange(0,24), diss_unfiltered, label = 'dissipation ', color= 'darkblue', linewidth = 3, linestyle = 'dotted')\n",
    "#ax1.plot(np.arange(0,24), max_hours_rain_unfiltered, label = 'rain_maxima ', color= 'darkblue', linewidth = 3, linestyle = '-')\n",
    "#ax1.plot(np.arange(0,24), max_hours_tb_unfiltered, label = 'tb_minima ', color= 'darkblue', linewidth = 3, linestyle = '-,')\n",
    "ax1.legend(fontsize=lf)\n",
    "\n",
    "a= np.arange(21,24,2)\n",
    "b= np.arange(1,20,2)\n",
    "localtime= np.append(a, b)\n",
    "\n",
    "labels=localtime.astype(str)\n",
    "ax1.set_yticks(np.arange(2,10,2))\n",
    "ax1.set_xticks(np.arange(0,23)[::2])\n",
    "ax1.set_yticklabels(ax1.get_yticks(),fontsize=f)\n",
    "ax1.set_xticklabels(labels,fontsize= f)\n",
    "ax1.set_xlim(-0.5,22.5)\n",
    "ax1.set_xlabel('Local time (UTC-3)', fontsize= l)\n",
    "ax1.set_ylabel('Frequency [%]', fontsize= l)\n",
    "\n",
    "\n",
    "labels=localtime.astype(str)\n",
    "ax2 = plt.subplot(2, 1, 2) \n",
    "ax2.set_title('b) Filtered for cold core and precip', loc='left', fontsize= 35 )\n",
    "ax2.plot(np.arange(0,24), init_filtered, label = 'initiation ', color= 'crimson', linewidth = 3)\n",
    "ax2.plot(np.arange(0,24), diss_filtered, label = 'dissipation ', color= 'crimson', linewidth = 3, linestyle = 'dotted')\n",
    "#ax2.plot(np.arange(0,24), max_hours_rain_filtered, label = 'rain_maxima ', color= 'crimson', linewidth = 3, linestyle = '-.',alpha=0.5)\n",
    "#ax2.plot(np.arange(0,24), max_hours_tb_filtered, label = 'tb_minima ', color= 'crimson', linewidth = 3, linestyle = '--',alpha=0.5)\n",
    "\n",
    "ax2.legend(fontsize=lf)\n",
    "ax2.set_yticks(np.arange(2,10,2))\n",
    "ax2.set_xticks(np.arange(0,23)[::2])\n",
    "ax2.set_xticklabels(labels,fontsize= f)\n",
    "ax2.set_xlim(-0.5,22.5)\n",
    "ax2.set_yticklabels(ax2.get_yticks(),fontsize=f)\n",
    "ax2.set_xlabel('Local time (UTC-3)', fontsize= l)\n",
    "ax2.set_ylabel('Frequency [%]', fontsize= l)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCS area plots\n",
    "# ==============\n",
    "area_unfiltered=functions.get_area(all_years_original)\n",
    "area_filtered=functions.get_area(all_years_CCPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_area_histo, bins = np.histogram(area_unfiltered, bins= np.linspace(60000,420000,10))\n",
    "f_area_histo, bins = np.histogram(area_filtered, bins= np.linspace(60000,420000,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of initiation time of MCSs \n",
    "import seaborn as sns \n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "t = 40\n",
    "f= 30\n",
    "n=25\n",
    "lf= 35\n",
    "\n",
    "plt.title('MCS area', loc='left', fontsize= t )\n",
    "plt.bar(bins[:-1],u_area_histo,width=35000*0.7,color= 'teal', label='original')\n",
    "plt.xlabel('MCS area (km$^{2}$)', fontsize= f)\n",
    "plt.xticks(bins.astype(int)[:-1], fontsize=n)\n",
    "#plt.xticks([60000,100000,140000,180000,220000,260000,300000,340000,380000,420000,460000], fontsize=n)\n",
    "#plt.yticks(np.arange(0,140)[::10],fontsize=n)\n",
    "#plt.yticks([0,20,40,60,80,100],fontsize=n)\n",
    "plt.yticks(fontsize=n)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "#ax1.xaxis.get_offset_text().set_fontsize(25)\n",
    "plt.ylabel('MCS tracks',fontsize=f)\n",
    "\n",
    "plt.bar(bins[:-1],f_area_histo, width=35000*0.05,color= 'orange',label='filtered')\n",
    "\n",
    "plt.legend(loc='best',fontsize=f)\n",
    "#fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_area_hist = u_area_histo/np.nansum(u_area_histo) * 100\n",
    "f_area_hist = f_area_histo/np.nansum(f_area_histo) * 100\n",
    "\n",
    "\n",
    "#plot of initiation time of MCSs \n",
    "import seaborn as sns \n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "t = 40\n",
    "f= 30\n",
    "n=25\n",
    "lf= 35\n",
    "\n",
    "plt.title('MCS area', loc='left', fontsize= t )\n",
    "plt.bar(bins[:-1],u_area_hist,width=35000*0.7,color= 'teal', label='original')\n",
    "plt.xlabel('MCS area (km$^{2}$)', fontsize= f)\n",
    "plt.xticks(bins.astype(int)[:-1], fontsize=n)\n",
    "#plt.xticks([60000,100000,140000,180000,220000,260000,300000,340000,380000,420000,460000], fontsize=n)\n",
    "plt.yticks(np.arange(0,140)[::10],fontsize=n)\n",
    "#plt.yticks([0,20,40,60,80,100],fontsize=n)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "#ax1.xaxis.get_offset_text().set_fontsize(25)\n",
    "plt.ylabel('Frequency [%]',fontsize=f)\n",
    "\n",
    "plt.bar(bins[:-1],f_area_hist, width=35000*0.05,color= 'orange',label='filtered')\n",
    "\n",
    "plt.legend(loc='best',fontsize=f)\n",
    "#fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots for velocity\n",
    "# ==================\n",
    "\n",
    "# Need to remane the columns to latitude and longitude (and -360 from the longitude) for velocity function and plotting to work \n",
    "all_years_original.rename(columns={'projection_x_coordinate': 'longitude', 'projection_y_coordinate': 'latitude'}, inplace=True)\n",
    "#all_years_CCPF.rename(columns={'projection_x_coordinate': 'longitude', 'projection_y_coordinate': 'latitude'}, inplace=True)\n",
    "\n",
    "all_years_original.longitude = all_years_original.longitude - 360\n",
    "#all_years_CCPF.longitude = all_years_CCPF.longitude - 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_CCPF.cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_CCPF = all_years_CCPF[all_years_CCPF.cell == 19983829]\n",
    "subset_CCPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_original = all_years_original[all_years_original.cell == 19981]\n",
    "subset_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_time = (subset_CCPF.time[subset_CCPF.feature == 15661] - subset_CCPF.time[subset_CCPF.feature == 15652]).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff_time = (feature_new[\"time\"] - feature_old[\"time\"]).total_seconds()\n",
    "#subset['time'].astype('timedelta64[ns]')\n",
    "\n",
    "diff_time = (subset_original.time[subset_original.feature == 12] - subset_original.time[subset_original.feature == 1]).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tobac\n",
    "\n",
    "#first need to run the tobac calculate velocity function before running my own function to create the histogram\n",
    "#vel_unfiltered= tobac.calculate_velocity(all_years_original, method_distance=None)\n",
    "vel_filtered = tobac.calculate_velocity(all_years_CCPF, method_distance=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Computes the Haversine distance in kilometers.\n",
    "\n",
    "    Calculates the Haversine distance between two points\n",
    "    (based on implementation CIS https://github.com/cedadev/cis).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat1, lon1 : array of latitude, longitude\n",
    "        First point or points as array in degrees.\n",
    "\n",
    "    lat2, lon2 : array of latitude, longitude\n",
    "        Second point or points as array in degrees.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    arclen * RADIUS_EARTH : array\n",
    "        Array of Distance(s) between the two points(-arrays) in\n",
    "        kilometers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    RADIUS_EARTH = 6378.0\n",
    "    lat1 = np.radians(lat1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    # print(lat1,lat2,lon1,lon2)\n",
    "    arclen = 2 * np.arcsin(\n",
    "        np.sqrt(\n",
    "            (np.sin((lat2 - lat1) / 2)) ** 2\n",
    "            + np.cos(lat1) * np.cos(lat2) * (np.sin((lon2 - lon1) / 2)) ** 2\n",
    "        )\n",
    "    )\n",
    "    return arclen * RADIUS_EARTH\n",
    "\n",
    "\n",
    "def calculate_distance(feature_1, feature_2, method_distance=None):\n",
    "    \"\"\"Compute the distance between two features. It is based on\n",
    "    either lat/lon coordinates or x/y coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_1, feature_2 : pandas.DataFrame or pandas.Series\n",
    "        Dataframes containing multiple features or pandas.Series\n",
    "        of one feature. Need to contain either projection_x_coordinate\n",
    "        and projection_y_coordinate or latitude and longitude\n",
    "        coordinates.\n",
    "\n",
    "    method_distance : {None, 'xy', 'latlon'}, optional\n",
    "        Method of distance calculation. 'xy' uses the length of the\n",
    "        vector between the two features, 'latlon' uses the haversine\n",
    "        distance. None checks wether the required coordinates are\n",
    "        present and starts with 'xy'. Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance : float or pandas.Series\n",
    "        Float with the distance between the two features in meters if\n",
    "        the input are two pandas.Series containing one feature,\n",
    "        pandas.Series of the distances if one of the inputs contains\n",
    "        multiple features.\n",
    "\n",
    "    \"\"\"\n",
    "    if method_distance is None:\n",
    "        if (\n",
    "            (\"projection_x_coordinate\" in feature_1)\n",
    "            and (\"projection_y_coordinate\" in feature_1)\n",
    "            and (\"projection_x_coordinate\" in feature_2)\n",
    "            and (\"projection_y_coordinate\" in feature_2)\n",
    "        ):\n",
    "            method_distance = \"xy\"\n",
    "        elif (\n",
    "            (\"latitude\" in feature_1)\n",
    "            and (\"longitude\" in feature_1)\n",
    "            and (\"latitude\" in feature_2)\n",
    "            and (\"longitude\" in feature_2)\n",
    "        ):\n",
    "            method_distance = \"latlon\"\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"either latitude/longitude or projection_x_coordinate/projection_y_coordinate have to be present to calculate distances\"\n",
    "            )\n",
    "\n",
    "    if method_distance == \"xy\":\n",
    "        distance = np.sqrt(\n",
    "            (\n",
    "                feature_1[\"projection_x_coordinate\"]\n",
    "                - feature_2[\"projection_x_coordinate\"]\n",
    "            )\n",
    "            ** 2\n",
    "            + (\n",
    "                feature_1[\"projection_y_coordinate\"]\n",
    "                - feature_2[\"projection_y_coordinate\"]\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "    elif method_distance == \"latlon\":\n",
    "        distance = 1000 * haversine(\n",
    "            feature_1[\"latitude\"],\n",
    "            feature_1[\"longitude\"],\n",
    "            feature_2[\"latitude\"],\n",
    "            feature_2[\"longitude\"],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"method undefined\")\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_velocity_individual(feature_old, feature_new, method_distance=None):\n",
    "    \"\"\"Calculate the mean velocity of a feature between two timeframes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_old : pandas.Series\n",
    "        pandas.Series of a feature at a certain timeframe. Needs to\n",
    "        contain a 'time' column and either projection_x_coordinate\n",
    "        and projection_y_coordinate or latitude and longitude coordinates.\n",
    "\n",
    "    feature_new : pandas.Series\n",
    "        pandas.Series of the same feature at a later timeframe. Needs\n",
    "        to contain a 'time' column and either projection_x_coordinate\n",
    "        and projection_y_coordinate or latitude and longitude coordinates.\n",
    "\n",
    "    method_distance : {None, 'xy', 'latlon'}, optional\n",
    "        Method of distance calculation, used to calculate the velocity.\n",
    "        'xy' uses the length of the vector between the two features,\n",
    "        'latlon' uses the haversine distance. None checks wether the\n",
    "        required coordinates are present and starts with 'xy'.\n",
    "        Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    velocity : float\n",
    "        Value of the approximate velocity.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    distance = calculate_distance(\n",
    "        feature_old, feature_new, method_distance=method_distance\n",
    "    )\n",
    "    diff_time = (feature_new[\"time\"] - feature_old[\"time\"]).total_seconds()\n",
    "    velocity = distance / diff_time\n",
    "    return velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new = subset_CCPF[subset_CCPF.feature == 15661]\n",
    "feature_new['time'].dtype\n",
    "feature_new\n",
    "\n",
    "feature_old = subset_CCPF[subset_CCPF.feature == 15652]\n",
    "feature_old['time'].dtype\n",
    "feature_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_original.time.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_time = (feature_new[\"time\"] - feature_old[\"time\"]).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_new = subset_original[subset_original.feature == 12]\n",
    "feature_new['time']\n",
    "#feature_new = pd.to_timedelta(feature_new.time)\n",
    "\n",
    "feature_old = subset_original[subset_original.feature == 1]\n",
    "feature_old['time'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_time = (feature_new[\"time\"] - feature_old[\"time\"]).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity = calculate_velocity_individual(subset_original[subset_original.feature == 12], subset_original[subset_original.feature == 1], method_distance=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_unfiltered= tobac.calculate_velocity(all_years_original, method_distance=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_CCPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mpl_toolkits\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "x_f = all_years_CCPF.longitude\n",
    "y_f = all_years_CCPF.latitude\n",
    "\n",
    "x_o = all_years_original.longitude\n",
    "y_o = all_years_original.latitude\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(22,12))\n",
    "\n",
    "ax1=plt.subplot(1,3,1)\n",
    "m = Basemap(llcrnrlon=-80, llcrnrlat=-38,urcrnrlon=-30, urcrnrlat=10) # create Basemap object\n",
    "m.drawcoastlines() # draw coastlines\n",
    "#m.drawcountries() # draw political boundaries\n",
    "m.shadedrelief()\n",
    "\n",
    "hx_o = m.hexbin(x_o, y_o, gridsize=(8,8),cmap='Reds',alpha=0.6, vmin=0,vmax=1000, extent=(-80, -30, -40, 15))\n",
    "cbar = plt.colorbar(hx_o, ax=ax1, shrink=0.4, label='Hours with MCS present', extend='max')\n",
    "ax1.set_xlabel('Lon $^\\circ$E', fontsize=15)\n",
    "ax1.set_ylabel('Lat $^\\circ$N', fontsize=15)\n",
    "ax1.set_title('MCS frequency in {} (original)'.format(year), fontsize=20)\n",
    "\n",
    "\n",
    "ax2=plt.subplot(1,3,2)\n",
    "m = Basemap(llcrnrlon=-80, llcrnrlat=-38,urcrnrlon=-30, urcrnrlat=10) # create Basemap object\n",
    "m.drawcoastlines() # draw coastlines\n",
    "#m.drawcountries() # draw political boundaries\n",
    "m.shadedrelief()\n",
    "\n",
    "hx_f = m.hexbin(x_f, y_f, gridsize=(8,8),cmap='Reds',alpha=0.6, vmin=0,vmax=1000, extent=(-80, -30, -40, 15))\n",
    "cbar = plt.colorbar(hx_f, ax=ax2, shrink=0.4, label='Hours with MCS present', extend='max')\n",
    "ax2.set_xlabel('Lon $^\\circ$E', fontsize=15)\n",
    "ax2.set_ylabel('Lat $^\\circ$N', fontsize=15)\n",
    "ax2.set_title('MCS frequency in {} (filtered)'.format(year), fontsize=20)\n",
    "\n",
    "\n",
    "ax3=plt.subplot(1,3,3)\n",
    "m = Basemap(llcrnrlon=-80, llcrnrlat=-38,urcrnrlon=-30, urcrnrlat=10) # create Basemap object\n",
    "m.drawcoastlines() # draw coastlines\n",
    "#m.drawcountries() # draw political boundaries\n",
    "m.shadedrelief()\n",
    "\n",
    "# Create dummy hexbin using whatever data..:\n",
    "hx_diff=m.hexbin(x_f, y_f, gridsize=(8,8), cmap='RdBu_r',alpha=0.6, vmin=-500,vmax=500, extent=(-80, -30, -40, 15))\n",
    "hx_diff.set_array(hx_f.get_array()-hx_o.get_array())\n",
    "cbar = plt.colorbar(hx_diff, ax=ax3, shrink=0.4, extend='both')\n",
    "cbar.set_label(label='filtered - original')\n",
    "#cbar.ax.tick_params(labelsize=12)\n",
    "ax3.set_xlabel('Lon $^\\circ$E', fontsize=15)\n",
    "ax3.set_ylabel('Lat $^\\circ$N', fontsize=15)\n",
    "ax3.set_title('Difference', fontsize=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
